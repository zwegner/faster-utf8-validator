// faster-utf8-validator
//
// Copyright (c) 2019 Zach Wegner
// 
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
// 
// The above copyright notice and this permission notice shall be included in
// all copies or substantial portions of the Software.
// 
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.

#include <stdint.h>

#include <assert.h>

#if defined(NEON)
#   include <arm_neon.h>
#else
#   include <immintrin.h>
#endif

// faster-utf8-validator: Technical Details
// ====
//
//   [[[ UTF-8 refresher: UTF-8 encodes text in sequences of "code points", each
//   one from 1-4 bytes. The one byte code points are ASCII (UTF-8 is backwards
//   compatible with ASCII). Each ASCII byte is in the range 0-127, with the
//   high bit unset (it's a 7-bit format). For each code point that is longer
//   than one byte, the code point begins with a unique prefix of bits that
//   specifies how many bytes follow. The first byte in these N-byte sequences
//   (I call these "leader bytes") have N 1's as their most significant bits.
//   All bytes in the code point after the leader byte have a continuation
//   marker in the top bits (10). All code points in UTF-8 will thus look like
//   one of the following binary sequences, with x meaning "don't care":
//      1 byte:  0xxxxxxx
//      2 bytes: 110xxxxx  10xxxxxx
//      3 bytes: 1110xxxx  10xxxxxx  10xxxxxx
//      4 bytes: 11110xxx  10xxxxxx  10xxxxxx  10xxxxxx
//   ]]]
//
// This validator works in two basic steps: checking continuation bytes, and
// handling special cases. The special cases are handled first, and some data
// from the special cases are used for continuation byte checking, so I'll
// explain that first.
// 
// Special cases/table lookups
// ====
//
// Besides the basic prefix coding of UTF-8 mentioned above, there are several
// invalid byte sequences that need to be detected. These are due to three
// factors: code points that could be described in fewer bytes ("overlong"),
// code points that are part of a "surrogate pair" (which are code points in a
// specific range that are used in UTF-16 to express other code points as a pair
// of 16-bit values, and are only valid in UTF-16), and code points that are
// past the highest valid code point U+10FFFF.
//
// At a high level, the special case checking works using the typical SIMD
// "shuffle" instruction. The way these instructions are used here are like
// parallel table lookups for several bytes at a time. For example, in SSE, AVX,
// and NEON, there are 16-way lookup instructions: for each byte of input in a
// vector, the low four bits are used as an index into a 16-byte lookup table
// (contained in another vector register). SSE and NEON can do this
// 4-bit/16-byte lookup for 16 bytes at a time, and AVX2 can do the same for 32
// bytes at a time (since it works like two copies of the SSE instruction
// executed side-by-side).
//
// All of the invalid special-case sequences can be detected by independently
// observing the first three nibbles of each code point, looking up an error
// mask from a 16 byte table for each using the above shuffle instructions.
// For example, for checking the byte sequence 0xC1 0x82, we do three table
// lookups, with the indices for the first code point being 0xC, 0x1, and 0x8.
// This is only for the code point starting at the first byte, though; we check
// several bytes at a time with these nifty SIMD instructions. The three nibbles
// of a code point span two different bytes, so we need to compare the table
// lookups from two adjacent bytes. For this, we get a vector with all the input
// bytes shifted forward by one (typically using an unaligned load). In this
// code, this is called "shifted_bytes".
//
// All of the tables for these lookups are generated by some Python code
// (gen_table.py), and are customized for each architecture.
//
// A small visualization of an 8-byte sequence, and the indices for each of the
// three nibble lookup-tables (all values in hexadecimal):
//
//   bytes:         C1 82 61 F0 90 80 80 00
//   shifted_bytes: 00 C1 82 61 F0 90 80 80
//   1st index:     0  C  8  6  F  9  8  8  
//   2nd index:      0  1  2  1  0  0  0  0 
//   3nd index:     C  8  6  F  9  8  8  0 
//
// With these three table indices, we look up three result bytes from three
// different tables for each contiguous grouping of three nibbles. We use each
// byte as eight separate 1-bit flags, that can be compared by bitwise AND
// across the three results. When the three result bytes all have an error bit
// in common, we flag the input as invalid.
//
// Taking into account the prefix encoding and the three special cases, we have
// these possible values for valid UTF-8 sequences, broken down by the first
// three nibbles:
//
//   1st   2nd   3rd   comment
//   0..7  0..F  -     ASCII
//   8..B  0..F  -     continuation bytes
//   C     2..F  8..B  C0 xx and C1 xx can be encoded in 1 byte
//   D     0..F  8..B  D0..DF are valid with a continuation byte
//   E     0     A..B  E0 8x and E0 9x can be encoded with 2 bytes
//         1..C  8..B  E1..EC are valid with continuation bytes
//         D     8..9  ED Ax and ED Bx correspond to surrogate pairs
//         E..F  8..B  EE..EF are valid with continuation bytes
//   F     0     9..B  F0 8x can be encoded with 3 bytes
//         1..3  8..B  F1..F3 are valid with continuation bytes
//         4     8     F4 8F BF BF is the maximum valid code point
//
// That leaves us with these invalid sequences, which would otherwise fit into
// UTF-8's prefix encoding. Each of these invalid sequences needs to be detected
// separately, with their own bits in the error mask. The symbolic name of each
// error bit is listed--these are defined in gen_table.py.
//
//   1st   2nd   3rd   error bit
//   C     0..1  0..F  ERR_OVER1
//   E     0     8..9  ERR_OVER2
//         D     A..B  ERR_SURR
//   F     0     0..8  ERR_OVER3
//         4     9..F  ERR_MAX1
//         5..F  0..F  ERR_MAX2
//
// For every possible value of the first, second, and third nibbles, we keep
// a lookup table that contains the bitwise OR of all errors that that nibble
// value can cause. For example, the first table has zeroes in every entry
// except for C, E, and F, and the third nibble table has the ERR_OVER1 and
// ERR_MAX2 bits set in every entry, since those errors don't depend on the
// third nibble. After doing all the parallel lookups, we AND the three vectors
// together. Only when all three have an error bit in common do we fail
// validation.
//
// Also, note that all of the details above change for AVX-512 support (with the
// VBMI extension). There, we have the awesome VPERMB instruction, which does
// a 64-byte table lookup in parallel for each of 64 input bytes (using the low
// six bits of each input byte), all in a mouth-watering three cycles. With the
// six index bits we have, we can fit three nibbles' worth of lookups into just
// two 64-way lookups. The indexing scheme is described in (much) more detail in
// gen_table.py, in the make_64_bit_tables() function.
//
// Extra info
// ====
//
// So in the above lookup tables, we only use six bits for errors. We have two
// extra bits that we can go nuts with, what should we do? (Hat tip to @aqrit
// on github for pointing this possibility out)
//
// Turns out we can use the extra bits for dealing with continuation bytes in
// two different ways, allowing us to entirely skip a separate check for 2-byte
// code points, and getting a mask of continuation bytes for free.
//
// These techniques are fairly arcane, and are described more in gen_table.py
// (search for ERR_CONT, MARK_CONT, and MARK_CONT2).
//
// Continuation bytes
// ====
//
// Well, compared to all that tricky lookup business, continuation bytes are a
// walk in the park. Depending on the architecture, this is done in either
// vector or scalar registers (see the USE_VECTOR_CONT_CHECK #define for
// details).
//
// In either case, this involves creating a mask of required continuation bytes
// by shifting the 3/4 byte markers from the lookup tables forward 3 and 4 bytes
// respectively, and comparing those markers with the continuation marker, also
// hidden in the lookup tables. This is the extra info mentioned above--note
// that we don't check for two byte sequences, and the continuation markers are
// only set for two consecutive continuation bytes (thus ignoring 2-byte
// sequences, which only have one continuation byte).
//
// Here's an example input with # of bytes labeled for each code point, the
// required continuation markers, and the actual continuation byte markers.
// Observe that the continuation markers line up with the 3-byte markers shifted
// forward by 2 bytes and the 4-byte markers shifted forward by 3, indicating
// that this is valid UTF-8.
//
//   bytes:        61 C3 80 62 E0 A0 80 63 F0 90 80 80 00
//   code points:  61|C3 80|62|E0 A0 80|63|F0 90 80 80|00
//   # of bytes:   1 |2  - |1 |3  -  - |1 |4  -  -  - |1
//   req. mark 3:  -  -  -  -  1  -  -  -  1  -  -  -  -
//     shift:                  >-----v     >-----v 
//   req. mark 4:  -  -  -  -  -  -  -  -  1  -  -  -  -
//     shift:                              >--------v 
//   cont. mark:   -  -  -  -  -  -  1  -  -  -  1  1  -
//
// Of course, this is different for AVX-512: there, we can't easily derive the
// 3/4 byte markers from the lookup tables, and instead we use unsigned byte
// comparisons with 0xE0 and 0xF0 respectively.
//
// Also, note that this needs special handling for when markers cross vector
// boundaries: when a 3/4 byte leader byte is at the end of a vector, we must
// make sure the next vector has the necessary continuation bytes. To do this we
// keep a "carry" mask of the bits that were shifted past the boundary in the
// last loop iteration.


// Some compile-time options for controlling the outer loop structure

#if !defined(USE_NEW_VECTOR_CONT_CHECK)
#  define USE_NEW_VECTOR_CONT_CHECK   (0)
#endif

// Load one vector ahead in the inner loop. This really shouldn't make a big
// difference, but at least with GCC 9 makes a strong positive impact.
#define USE_NEXT_LOAD           !USE_NEW_VECTOR_CONT_CHECK
// Whether to get shifted_bytes with an unaligned load. Otherwise, it is
// constructed with a vpalignr (or similar) instruction with two consecutive
// vectors. Only used if USE_NEXT_LOAD is undefined.
#define USE_UNALIGNED_LOADS     (1)
// How many vector's worth of input to process at a time in the inner loop.
// ASCII early exits and error checking are only performed once for each chunk.
#define UNROLL_COUNT            (6)

#define UNROLL_SIZE             (UNROLL_COUNT * V_LEN)

#if USE_NEW_VECTOR_CONT_CHECK && (USE_UNALIGNED_LOADS || USE_NEXT_LOAD || USE_VECTOR_CONT_CHECK)
#  error "Incompatible options"
#endif

// Whether to do the continuation byte check in scalar or vector code
// NEON doesn't have a movemask instruction, so it's better there to directly
// compare in vector registers.
// SSE has a movemask instruction, but it also has the cheap VPALIGNR
// instruction, so that vector registers can be shifted easily.
// AVX2 can only align within 16-byte lanes, and thus needs an extra
// VPERM2I128 instruction for shifting vectors.
// I believe AVX-512 needs to use a VPERMT2B instruction for shifting.
// Thus for AVX2 and AVX-512 it's faster to use scalar shifts (in my tests).
#if !defined(USE_VECTOR_CONT_CHECK)
#  if !USE_NEW_VECTOR_CONT_CHECK && (defined(NEON) || defined(SSE4))
#   define USE_VECTOR_CONT_CHECK    (1)
#  else
#   define USE_VECTOR_CONT_CHECK    (0)
#  endif
#endif

// Various macros for GNU C extensions
#define LIKELY(x)           __builtin_expect((x), (1))
#define UNLIKELY(x)         __builtin_expect((x), (0))
#define UNUSED              __attribute__((unused))
#define ALIGNED(x)          __attribute__((aligned(x)))

#define DEBUG(x)            //x

#if defined(ASCII_CHECK)
#   define ASCII_SUFFIX     _ascii
#else
#   define ASCII_SUFFIX
#endif

// Dumb C preprocessor nonsense to get customized symbols from the configuration
#define NAME_(name, suff, ascii)    name##_##suff##ascii
#define NAME__(name, suff, ascii)   NAME_(name, suff, ascii)
#define NAME(name)                  NAME__(name, SUFFIX, ASCII_SUFFIX)

////////////////////////////////////////////////////////////////////////////////
// Platform-specific configuration /////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////

#if defined(AVX2)

////////////////////////////////////////////////////////////////////////////////
// AVX2 ////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////

#   define SUFFIX           avx2

#   define V_LEN            (32)

// Vector and vector mask types. We use #defines instead of typedefs so this
// header can be included multiple times with different configurations

#   define vec_t            __m256i
#   if USE_VECTOR_CONT_CHECK
#       define vmask_t      vec_t
#   else
#       define vmask_t      uint32_t
#       define vmask2_t     uint64_t
#   endif

#   define v_load(x)        _mm256_load_si256((vec_t *)(x))
#   define v_loadu(x)       _mm256_lddqu_si256((vec_t *)(x))
#   define v_set1           _mm256_set1_epi8
#   define v_and            _mm256_and_si256
#   define v_andn           _mm256_andnot_si256
#   define v_or             _mm256_or_si256
#   define v_xor            _mm256_xor_si256
#   define v_add            _mm256_add_epi32
#   define v_usubs          _mm256_subs_epu8
#   define v_sgt            _mm256_cmpgt_epi8
#   define v_insert         _mm256_insert_epi8
#   define v_shl(x, shift)  ((shift) ? _mm256_slli_epi16((x), (shift)) : (x))
#   define v_shr(x, shift)  ((shift) ? _mm256_srli_epi16((x), (shift)) : (x))

#   define v_test_any(x)    !_mm256_testz_si256((x), (x))
#   define v_test_bit(input, bit)                                           \
        _mm256_movemask_epi8(v_shl((input), 7 - (bit)))

// Parallel table lookup for all bytes in a vector. We need to AND with 0x0F
// for the lookup, because vpshufb has the neat "feature" that negative values
// in an index byte will result in a zero.

#   define v_lookup(table, index)   _mm256_shuffle_epi8((table), (index))

#   define V_TABLE_32(...)    _mm256_setr_epi8(__VA_ARGS__)

// Simple macro to make a vector lookup table for use with vpshufb. Since
// AVX2 is two 16-byte halves, we duplicate the input values.

#   define V_TABLE_16(...)    V_TABLE_32(__VA_ARGS__, __VA_ARGS__)

// Move all the bytes in "input" to the left by one and fill in the first
// byte with zero. Since AVX2 generally works on two separate 16-byte
// vectors glued together, this needs two steps. The permute2x128 takes the
// middle 32 bytes of the 64-byte concatenation v_zero:input. The align
// then gives the final result in each half:
//      top half: input_L:input_H --> input_L[15]:input_H[0:14]
//   bottom half:  zero_H:input_L -->  zero_H[15]:input_L[0:14]

static UNUSED inline vec_t NAME(v_shift_lanes)(vec_t bottom, vec_t top,
        const uint32_t n) {
    // XXX can't use a shift from a variable at all, even when this is inlined
    // as a constant
    vec_t shl_16 = _mm256_permute2x128_si256(top, bottom, 0x03);
    if (n == 0)
        return top;
    else if (n == 1)
        return _mm256_alignr_epi8(top, shl_16, 16 - 1);
    else if (n == 2)
        return _mm256_alignr_epi8(top, shl_16, 16 - 2);
    else if (n == 3)
        return _mm256_alignr_epi8(top, shl_16, 16 - 3);
    else
        assert(!"unsupported lane shift");
}

static UNUSED inline vec_t NAME(v_load_shift_first)(const char *data,
        char first) {
    return NAME(v_shift_lanes)(v_set1(first), v_load(data), 1);
}

#elif defined(AVX512_VBMI)

////////////////////////////////////////////////////////////////////////////////
// AVX512 //////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////

#   define SUFFIX           avx512_vbmi

#   define V_LEN            (64)

// Double mask struct. We manage the low and high halves manually since
// LLVM/GCC were generating crappy code for uint128_t.
typedef struct {
    __mmask64 lo, hi;
} NAME(vmask2_t);

#   define vec_t            __m512i
#   define vmask_t          __mmask64
#   define vmask2_t         NAME(vmask2_t)

#   define v_load(x)        _mm512_load_si512((vec_t *)(x))
#   define v_loadu(x)       _mm512_loadu_si512((vec_t *)(x))
#   define v_set1           _mm512_set1_epi8
#   define v_and            _mm512_and_si512
#   define v_or             _mm512_or_si512
#   define v_shr(x, shift)  _mm512_srli_epi16((x), (shift))

#   define v_test_any(x)    _mm512_test_epi8_mask((x), (x))
#   define v_test_bit(input, bit)                                           \
        _mm512_test_epi8_mask((input), v_set1((uint32_t)1 << (bit)))

#   define v_lookup_64(table, index)                                    \
        _mm512_permutexvar_epi8((index), (table))

// Same macro as for AVX2, but repeated four times

#   define V_TABLE_64(...)    _mm512_setr_epi8(__VA_ARGS__)

// Hack around setr_epi8 not being available. I'm not sure how widely supported
// this syntax is (at least GCC and LLVM work)...
#   define _mm512_setr_epi8(...) \
        (__extension__ (vec_t)(__v64qi) { __VA_ARGS__ } )

static inline vec_t NAME(v_shift_lanes)(vec_t bottom, vec_t top,
        const uint32_t n) {
    (void)n;
    vec_t shift_indices = _mm512_setr_epi8(
        127,0, 1, 2, 3, 4, 5, 6,
         7, 8, 9,10,11,12,13,14,
        15,16,17,18,19,20,21,22,
        23,24,25,26,27,28,29,30,
        31,32,33,34,35,36,37,38,
        39,40,41,42,43,44,45,46,
        47,48,49,50,51,52,53,54,
        55,56,57,58,59,60,61,62
    );
    return _mm512_permutex2var_epi8(top, shift_indices, bottom);
}

// Load from the "data" pointer, but shifted one byte forward. We want to do
// this without touching memory before the pointer, since it might be unmapped.
// Rather than mucking around with permutes or something to do this, we can use
// a mask register to load starting from [data - 1], without actually loading
// into the first byte of the vector (which is set to "first" due to the _mask
// load variant). This will not fault if [data - 1] is invalid memory. Intel's
// docs are rather vague here, just mentioning that masked loads have "fault
// suppression", but this in fact means that lanes not in the mask cannot
// trigger page faults. See: https://stackoverflow.com/questions/54497141
// Thanks to Travis Downs for this idea.
static inline vec_t NAME(v_load_shift_first)(const char *data, char first) {
    vec_t result = v_set1(first);
    // All bits but the first
    __mmask64 shift_mask = ~1ULL;
    return _mm512_mask_loadu_epi8(result, shift_mask, data - 1);
}

#elif defined(SSE4)

////////////////////////////////////////////////////////////////////////////////
// SSE4 definitions ////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////

#   define SUFFIX           sse4

#   define V_LEN            (16)

#   define vec_t            __m128i
#   if USE_VECTOR_CONT_CHECK
#       define vmask_t      vec_t
#   else
#       define vmask_t      uint16_t
#       define vmask2_t     uint32_t
#   endif

#   define v_load(x)        _mm_load_si128((vec_t *)(x))
#   define v_loadu(x)       _mm_lddqu_si128((vec_t *)(x))
#   define v_set1           _mm_set1_epi8
#   define v_and            _mm_and_si128
#   define v_andn           _mm_andnot_si128
#   define v_or             _mm_or_si128
#   define v_xor            _mm_xor_si128
#   define v_add            _mm_add_epi32
#   define v_usubs          _mm_subs_epu8
#   define v_sgt            _mm_cmpgt_epi8
#   define v_insert         _mm_insert_epi8
#   define v_shl(x, shift)  ((shift) ? _mm_slli_epi16((x), (shift)) : (x))
#   define v_shr(x, shift)  ((shift) ? _mm_srli_epi16((x), (shift)) : (x))

#   define v_test_any(x)    !_mm_test_all_zeros((x), (x))
#   define v_test_bit(input, bit)                                           \
        _mm_movemask_epi8(v_shl((input), (uint8_t)(7 - (bit))))

#   define v_lookup(table, index)   _mm_shuffle_epi8((table), (index))

#   define V_TABLE_16(...)  _mm_setr_epi8(__VA_ARGS__)

static UNUSED inline vec_t NAME(v_shift_lanes)(vec_t bottom, vec_t top,
        const uint32_t n) {
    // XXX can't use a shift from a variable at all, even when this is inlined
    // as a constant
    if (n == 0)
        return top;
    else if (n == 1)
        return _mm_alignr_epi8(top, bottom, 16 - 1);
    else if (n == 2)
        return _mm_alignr_epi8(top, bottom, 16 - 2);
    else if (n == 3)
        return _mm_alignr_epi8(top, bottom, 16 - 3);
    else
        assert(!"unsupported lane shift");
}

static UNUSED inline vec_t NAME(v_load_shift_first)(const char *data,
        char first) {
    return NAME(v_shift_lanes)(v_set1(first), v_load(data), 1);
}

#elif defined(NEON)

////////////////////////////////////////////////////////////////////////////////
// NEON definitions ////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////

#   define SUFFIX           neon

#   define V_LEN            (16)

// For NEON, since there is no movemask-like instruction, we keep continuation
// byte errors in a regular vector. There is no vmask2_t needed.
#   define vec_t            uint8x16_t
#   define vmask_t          uint8x16_t

#   define v_load(x)        vld1q_u8((uint8_t *)(x))
#   define v_loadu(x)       vld1q_u8((uint8_t *)(x))
#   define v_set1           vdupq_n_u8
#   define v_and            vandq_u8
#   define v_andn(a,b)      vbicq_u8((b), (a))
#   define v_or             vorrq_u8
#   define v_xor            veorq_u8
#   define v_shl(x, shift)  ((shift) ? vshlq_n_u8((x), (shift)) : (x))
#   define v_shr(x, shift)  ((shift) ? vshrq_n_u8((x), (shift)) : (x))

#   define v_test_any       NAME(_v_test_any)
#   define v_test_bit(input, bit)                                           \
        v_test_any(v_and((input), v_set1((uint8_t)(1 << (bit)))))

static inline uint64_t NAME(_v_test_any)(vec_t vec) {
    uint64x2_t vec_64 = (uint64x2_t)vec;
    return vgetq_lane_u64(vec_64, 0) | vgetq_lane_u64(vec_64, 1);
}

#   define v_lookup(table, index)   vqtbl1q_u8((table), (index))

#   define V_TABLE_16(...)  ( (uint8x16_t) { __VA_ARGS__ } )

static inline vec_t NAME(v_shift_lanes)(vec_t bottom, vec_t top,
        const uint32_t n) {
    return vextq_u8(bottom, top, 16 - n);
}

static UNUSED inline vec_t NAME(v_load_shift_first)(const char *data, char first) {
    return NAME(v_shift_lanes)(v_set1(first), v_load(data), 1);
}

#else

#   error "No valid configuration: must define one of AVX512_VBMI, " \
        "AVX2, SSE4, or NEON"

#endif

// Result struct. We combine special-case test errors and continuation byte
// errors into one result, so we can easily unroll the inner loop and only
// branch once for several vectors' worth of results.
#define result_t    NAME(_result_t)
typedef struct {
#if USE_NEW_VECTOR_CONT_CHECK
    vec_t error;
#else
    vec_t lookup_error;
    vmask_t cont_error;
#endif
} result_t;

// Some code paths for result checking are common between all x86 ISAs and
// different on NEON. These are collected here.
#if USE_NEW_VECTOR_CONT_CHECK

#   define vmask_zero()     (0)
#   if V_LEN == 16
#       define is_incomplete(r)                                            \
            v_usubs((r), V_TABLE_16(-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1, \
                        0xEF, 0xDF, 0xBF))
#   elif V_LEN == 32
#       define is_incomplete(r)                                            \
            v_usubs((r), V_TABLE_32(-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,\
                        -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,      \
                        0xEF, 0xDF, 0xBF))
#   endif

#   define test_carry_req(s)    v_test_any(is_incomplete(s->last_bytes))
#   define result_fails(r)      UNLIKELY(v_test_any((r).error))
#   define combine_results(r1, r2) do { \
        r1.error = v_or(r1.error, r2.error); \
    } while (0)

#elif USE_VECTOR_CONT_CHECK

#   define vmask_zero()     v_set1(0)
// In the NEON code paths, we keep continuation byte masks as vectors, and need
// a special test at the end of the input to make sure we weren't expecting
// more continuation bytes. This means a 4-byte sequence starting at the second
// to last position, or a 3-byte sequence starting at the last position. Since
// the continuation marker bits are already shifted forward by one (due to
// coming from the special case error lookups), the last two positions are
// sufficient to catch any expected continuation bytes at the end.
#   if V_LEN == 16
#       define mask_carry_req(r)                                            \
            v_and((r), V_TABLE_16(0,0,0,0,0,0,0,0,0,0,0,0,0,0,              \
                        1u << MARK_4_BYTE, 1u << MARK_4_BYTE | 1u << MARK_3_BYTE))
#   elif V_LEN == 32
#       define mask_carry_req(r)                                            \
            v_and((r), V_TABLE_32(0,0,0,0,0,0,0,0,0,0,0,0,0,0,              \
                        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,                    \
                        1u << MARK_4_BYTE, 1u << MARK_4_BYTE | 1u << MARK_3_BYTE))
#   endif

#   define test_carry_req(s)    v_test_any(mask_carry_req(s->carry_req))
// Mask out everything but the MARK_CONT bit from the cont_error. This is
// just hoisting out an AND from the unrolled loop that the compiler can't
// really be trusted to do itself.
#   define result_fails(r)                                                  \
            (UNLIKELY(v_test_bit((r).cont_error, MARK_CONT)) ||             \
             UNLIKELY(v_test_any((r).lookup_error)))

#   define combine_results(r1, r2) do { \
        r1.lookup_error = v_or(r1.lookup_error, r2.lookup_error); \
        r1.cont_error = v_or(r1.cont_error, r2.cont_error); \
    } while (0)

#else

#   define vmask_zero()         (0)
#   define mask_carry_req(r)    (r)
#   define test_carry_req(s)    ((s->carry_req) != 0)
#   define result_fails(r)                                                  \
            (UNLIKELY((r).cont_error != 0) ||                               \
             UNLIKELY(v_test_any((r).lookup_error)))

#   define combine_results(r1, r2) do { \
        r1.lookup_error = v_or(r1.lookup_error, r2.lookup_error); \
        r1.cont_error = r1.cont_error | r2.cont_error; \
    } while (0)

#endif

#define state_t     NAME(_state_t)
typedef struct {
    vec_t bytes;
    vec_t shifted_bytes;
    // Unused unless USE_NEXT_LOAD is true
    vec_t next_shifted_bytes;
    // Unused unless USE_NEXT_LOAD and USE_UNALIGNED_LOADS are both false
    vec_t last_bytes;
    // Keep continuation bits from the previous iteration that carry over to
    // each input chunk vector
    vmask_t carry_req;
} state_t;

#include <stdio.h>
static void UNUSED NAME(print_vec)(vec_t a) {
    char ALIGNED(V_LEN) buf[V_LEN];
    *(vec_t *)buf = a;
    printf("{");
    for (uint32_t i = 0; i < V_LEN; i++)
        printf("%2x,", buf[i] & 0xff);
    printf("}\n");
}
#define PRINT 0

static inline void NAME(init_state)(state_t *state) {
    state->carry_req = vmask_zero();

    if (!USE_NEXT_LOAD && !USE_UNALIGNED_LOADS)
        state->last_bytes = v_set1(0);
}

static inline void NAME(load_first)(UNUSED state_t *state,
        UNUSED const char *data, UNUSED size_t pre_len) {
    if (USE_NEXT_LOAD)
        state->next_shifted_bytes = NAME(v_load_shift_first)(data,
                pre_len > 0 ? data[-1] : '\0');
    else if (!USE_UNALIGNED_LOADS) {
        vec_t bytes = v_set1(0);

        if (pre_len > 3)
            pre_len = 3;
        switch (pre_len) {
            case 3: bytes = v_insert(bytes, data[-3], V_LEN - 3); // FALLTHROUGH
            case 2: bytes = v_insert(bytes, data[-2], V_LEN - 2); // FALLTHROUGH
            case 1: bytes = v_insert(bytes, data[-1], V_LEN - 1);
        }
        state->bytes = bytes;
    }
}

static inline UNUSED void NAME(load_next)(UNUSED state_t *state,
        UNUSED const char *data) {
    if (USE_NEXT_LOAD)
        state->next_shifted_bytes = v_loadu(data + V_LEN - 1);
    else if (!USE_UNALIGNED_LOADS) {
        state->bytes = v_load(data);
        if (USE_NEW_VECTOR_CONT_CHECK)
            state->last_bytes = state->bytes;
    }
}

static inline void NAME(load_data)(state_t *state, const char *data) {
    if (USE_NEXT_LOAD) {
#if PRINT
        puts("PRELOAD");
        NAME(print_vec)(state->last_bytes);
        NAME(print_vec)(state->bytes);
        NAME(print_vec)(state->shifted_bytes);
        puts("");
#endif

        state->bytes = v_load(data);
        state->shifted_bytes = state->next_shifted_bytes;
        state->next_shifted_bytes = v_loadu(data + V_LEN - 1);

#if PRINT
        NAME(print_vec)(state->last_bytes);
        NAME(print_vec)(state->bytes);
        NAME(print_vec)(state->shifted_bytes);
        puts("");
#endif
    } else if (USE_UNALIGNED_LOADS) {
        state->shifted_bytes = v_loadu(data - 1);
        state->bytes = v_load(data);
    } else {
        state->last_bytes = state->bytes;
        state->bytes = v_load(data);
        state->shifted_bytes = NAME(v_shift_lanes)(state->last_bytes,
                state->bytes, 1);
#if PRINT
        puts("LOAD");
        NAME(print_vec)(state->last_bytes);
        NAME(print_vec)(state->bytes);
        puts("");
#endif
    }
}

// Validate one vector's worth of input bytes
static inline result_t NAME(z_validate_vec)(state_t *state) {
    result_t result;

    // Add error masks as locals
#include "table.h"

#if defined(AVX512_VBMI)
    vmask2_t req = { state->carry_req, 0 };

    // Look up error masks for the two 6-bit indices
    vec_t e_1 = v_lookup_64(error_1, state->shifted_bytes);
    // Bitwise select: we want to combine the top two bits from shifted_bytes
    // with the bottom four bits from bytes, which we can do with one ternary
    // logic operation. 0xCA is an 8x1 bit lookup table, equivalent to
    // (a ? c : b). Using 0xF0 as the selector (a), we select between the
    // appropriately-shifted values of shifted_bytes (b) or bytes (c).
    vec_t index_2 = _mm512_ternarylogic_epi32(v_set1(0xF0),
            v_shr(state->shifted_bytes, 2), v_shr(state->bytes, 4), 0xCA);
    vec_t e_2 = v_lookup_64(error_2, index_2);

    // Check if any bits are set in both error masks
    result.lookup_error = v_and(e_1, e_2);

    // Get a bitmask of all continuation bytes in the input. We can cheat a bit
    // (in a fun way) by hiding the bit in the error lookup tables.
    vmask_t cont = v_test_bit(e_2, MARK_CONT);

    // Find 3/4-byte leader bytes. Since we use a different lookup table
    // structure than the 16-entry AVX2/SSE4/NEON code paths, we can't hide the
    // relevant bits in the error tables. So just use unsigned byte compares,
    // which should be quite similar in speed to pulling the bits out anyways.
    for (int n = 2; n <= 3; n++) {
        vmask_t set = _mm512_cmpge_epu8_mask(state->bytes, v_set1(0xFF << (7-n)));

        // Add shifted bits for required continuation bytes
        req.lo += set << n;
        req.hi += set >> (64 - n);
    }

    // Save required continuation bits for the next round
    state->carry_req = req.hi;

    result.cont_error = (cont ^ req.lo);

#else

    // Look up error masks for three consecutive nibbles. Note that we need a
    // special trick for the second nibble (as described in gen_table.py for
    // the MARK_CONT2 bit). There, we invert shifted_bytes and AND with 0x8F
    // with one AND NOT instruction, which zeroes out e_2 for ASCII input.
    vec_t e_1 = v_lookup(error_1, v_and(v_shr(state->shifted_bytes, 4), v_set1(0x0F)));
    //vec_t e_2 = v_lookup(error_2, v_andn(state->shifted_bytes, v_set1(0xFF)));
    vec_t e_2 = v_lookup(error_2, v_and(state->shifted_bytes, v_set1(0x0F)));
    vec_t e_3 = v_lookup(error_3, v_and(v_shr(state->bytes, 4), v_set1(0x0F)));

    // Get error bits common between the first and third nibbles. This is a
    // subexpression used for ANDing all three nibbles, but is also used for
    // finding continuation bytes after the first. The MARK_CONT bit is only
    // set in this mask if both the first and third nibbles correspond to
    // continuation bytes, so the first continuation byte after a leader byte
    // won't be checked.
    vec_t e_1_3 = v_and(e_1, e_3);

    // Create the result vector with any bits set in all three error masks.
    // Note that we use AND NOT here, because the bits in e_2 are inverted--
    // this is needed for ASCII->continuation to trigger the MARK_CONT2 error.
    //result.lookup_error = v_andn(e_2, e_1_3);
# if !USE_NEW_VECTOR_CONT_CHECK
    result.lookup_error = v_and(e_2, e_1_3);
# endif

#if PRINT
    NAME(print_vec)(state->last_bytes);
    NAME(print_vec)(state->bytes);
    NAME(print_vec)(state->shifted_bytes);
    NAME(print_vec)(e_1);
    NAME(print_vec)(e_2);
    NAME(print_vec)(e_3);
    NAME(print_vec)(result.lookup_error);
    puts("");
#endif

# if USE_NEW_VECTOR_CONT_CHECK

    vec_t shift_2 = NAME(v_shift_lanes)(state->last_bytes, state->bytes, 2);
    vec_t shift_3 = NAME(v_shift_lanes)(state->last_bytes, state->bytes, 3);
    vec_t v_3_4 = v_or(v_usubs(shift_2, v_set1(0xDF)), v_usubs(shift_3, v_set1(0xEF)));
    vec_t req_3_4 = v_and(v_sgt(v_3_4, v_set1(0)), v_set1((uint32_t)1 << MARK_CONT));
    result.error = v_xor(v_and(e_2, e_1_3), req_3_4);

#if PRINT
    NAME(print_vec)(state->last_bytes);
    NAME(print_vec)(state->bytes);
    NAME(print_vec)(state->shifted_bytes);
    NAME(print_vec)(shift_2);
    NAME(print_vec)(shift_3);
    NAME(print_vec)(v_3_4);
    NAME(print_vec)(req_3_4);
    NAME(print_vec)(result.error);
    puts("");
#endif

# elif USE_VECTOR_CONT_CHECK

    vec_t shift_1 = NAME(v_shift_lanes)(state->carry_req, e_1, 1);
    vec_t shift_2 = NAME(v_shift_lanes)(state->carry_req, e_1, 2);
    // Shift the 3/4 byte marker bits into place for the MARK_CONT bit,
    // and add them together. This is safe for similar reasons to the +=
    // explanation in the below x86 path, and because we use MARK_CONT==1<<0
    // on NEON (so other bits in shift_1/2 don't overflow into the MARK_CONT
    // bit). We don't AND with the MARK_CONT bit here before setting the bits
    // in result.cont_error, mainly because some compilers (at least GCC 8)
    // aren't quite smart enough to hoist the AND out of the unrolled loop.
    // We instead put the AND in the final result_fails macro.
#   if defined(NEON)

    if (1) {
        vec_t req_3 = v_shr(shift_1, MARK_3_BYTE - MARK_CONT);
        vec_t req_3_4 = vsraq_n_u8(req_3, shift_2, MARK_4_BYTE - MARK_CONT);

        result.cont_error = veorq_u8(req_3_4, e_1_3);
    } else {
        // XXX This saves a full vector instruction but is slower?? There is
        // a longer dependency chain...
        vec_t err_3 = vsraq_n_u8(e_1_3, shift_1, MARK_3_BYTE - MARK_CONT);
        result.cont_error = vsraq_n_u8(err_3, shift_2, MARK_4_BYTE - MARK_CONT);
    }

#   else

#     if MARK_CONT - MARK_4_BYTE != 1
#       error "bad bit numbering"
#     endif

    vec_t req_3 = v_shl(shift_1, MARK_CONT - MARK_3_BYTE);
    vec_t req_3_4 = v_or(req_3, v_add(shift_2, shift_2));

    result.cont_error = v_xor(req_3_4, e_1_3);

#   endif

    state->carry_req = e_1;

# else

    // req is a mask of what bytes are required to be continuation bytes after
    // the first, and cont is a mask of the continuation bytes after the first
    vmask2_t req = state->carry_req;
    vmask_t cont = v_test_bit(e_1_3, MARK_CONT);

    // Compute the continuation byte mask by finding bytes that start with
    // 11x, 111x, and 1111. For each of these prefixes, we get a bitmask
    // and shift it forward by 1, 2, or 3. This loop should be unrolled by
    // the compiler, and the (n == 1) branch inside eliminated.
    vmask_t leader_3 = v_test_bit(e_1, MARK_3_BYTE);
    // Micro-optimization: use x+x instead of x<<1, it's a tiny bit faster
    vmask_t leader_4 = v_test_bit(v_add(e_1, e_1), MARK_4_BYTE + 1);

    // We add the shifted mask here instead of ORing it, which would
    // be the more natural operation, so that this line can be done
    // with one lea. While adding could give a different result due
    // to carries, this will only happen for invalid UTF-8 sequences,
    // and in a way that won't cause it to pass validation. Reasoning:
    // Any bits for required continuation bytes come after the bits
    // for their leader bytes, and are all contiguous. For a carry to
    // happen, two of these bit sequences would have to overlap. If
    // this is the case, there is a leader byte before the second set
    // of required continuation bytes (and thus before the bit that
    // will be cleared by a carry). This leader byte will not be
    // in the continuation mask, despite being required. QEDish.
    req += (vmask2_t)leader_4 << 2;
    req += (vmask2_t)leader_3 << 1;

    // Save continuation bits and input bytes for the next round
    state->carry_req = req >> V_LEN;

    // Check that continuation bytes match. We must cast req from vmask2_t
    // (which holds the carry mask in the upper half) to vmask_t, which
    // zeroes out the upper bits
    result.cont_error = (cont ^ (vmask_t)req);

#endif

#endif

    return result;
}

// Validate a chunk of input data which is <UNROLL_COUNT> consecutive vectors.
// This assumes that <data> is aligned to a V_LEN boundary, and that we can read
// one byte before data. We only check for validation failures or ASCII-only
// input once in this function, for entire input chunks.
static inline int NAME(z_validate_unrolled_chunk)(state_t *state,
        const char *data) {
#if defined(ASCII_CHECK)
    // Quick skip for ASCII-only input. If there are no bytes with the high bit
    // set, we can skip this chunk. If we expected any continuation bytes here,
    // we return invalid, otherwise just skip it. Note that we have additional
    // duplicated loads here that ideally should be kept in registers for the
    // full validation loop below, which loads the same memory.
    vec_t ascii_vec = v_load(data);
    for (uint32_t i = 1; i < UNROLL_COUNT; i++)
        ascii_vec = v_or(ascii_vec, v_load(data + i * V_LEN));

    if (LIKELY(!v_test_bit(ascii_vec, 7))) {
        if (UNLIKELY(test_carry_req(state)))
            return 1;

        // Set up the state for the next iteration by loading the last
        // vector. This is rather ugly...
        NAME(load_next)(state, data + (UNROLL_COUNT - 1) * V_LEN);

        return 0;
    }
#endif

    // Run other validations. Annoyingly, at least one compiler (GCC 8)
    // doesn't optimize v_or(0, x) into x, so manually unroll the first
    // iteration
    NAME(load_data)(state, data + 0 * V_LEN);
    result_t result = NAME(z_validate_vec)(state);
    for (uint32_t i = 1; i < UNROLL_COUNT; i++) {
        NAME(load_data)(state, data + i * V_LEN);
        result_t r = NAME(z_validate_vec)(state);

        combine_results(result, r);
    }

    return result_fails(result);
}

// Validate a piece of data too small to fit in a full unrolled chunk. This is
// done at the beginning and end of the input, and is slower because we copy
// input data into a buffer--we need to copy because memory outside the input
// might be unmapped and cause a segfault. This function takes two nontrivial
// parameters: <first> is the character before the input data pointer (which is
// handled specially outside of this code), and <align_at_start>, which decides
// which side of the buffer to align the input data on (the end of the buffer
// for the first part of the data, or the start of the buffer for the last part
// of the data). This is so the transient state such as the carry_req mask works
// consistently across the entire input--it can be thought of like padding the
// input data with NUL bytes on either side.
static inline int NAME(z_validate_small_chunk)(state_t *state, const char *data,
        ssize_t len, ssize_t pre_len, int align_at_start) {
    // Deal with any bytes remaining. Rather than making a separate scalar path,
    // just fill in a buffer, reading bytes only up to len, and load from that.
    char ALIGNED(V_LEN) buffer[2 * V_LEN] = { 0 };
    size_t offset = align_at_start ? V_LEN : 2 * V_LEN - len;
    assert(len <= V_LEN);
    if (pre_len > 3)
        pre_len = 3;
    for (ssize_t i = -pre_len; i < len; i++)
        buffer[offset + i] = data[i];

#if PRINT
    printf("DATA: ");
    for (ssize_t i = 0; i < len; i++)
        printf("%2x,", data[i] & 0xff);
    printf("\nPRE: ");
    for (ssize_t i = 0; i < pre_len; i++)
        printf("%2x,", data[-i-1] & 0xff);
    printf("\n{");
    for (ssize_t i = 0; i < V_LEN*2; i++)
        printf("%2x,", buffer[i] & 0xff);
    printf("} off=%lu\n\n", offset);
#endif

    state->last_bytes = v_load(buffer);
    state->bytes = v_load(buffer + V_LEN);
    state->shifted_bytes = v_loadu(buffer + V_LEN - 1);
    result_t result = NAME(z_validate_vec)(state);
    return result_fails(result);
}

int NAME(z_validate_utf8)(const char *data, size_t len) {
    state_t state[1];
    NAME(init_state)(state);

#if USE_NEXT_LOAD || !USE_UNALIGNED_LOADS
    // Get an aligned pointer to our input data. We round up to the next
    // multiple of V_LEN after data.
    intptr_t aligned_data_i = ((intptr_t)data + V_LEN - 1) & -V_LEN;
#else
    // Get an aligned pointer to our input data. We round up to the next
    // multiple of V_LEN after data + 1, since we need to read one byte before
    // the data pointer for shifted_bytes. This rounding is equivalent to
    // rounding down after adding V_LEN, which is what this does, by clearing
    // the low bits of the pointer (V_LEN is always a power of two).
    intptr_t aligned_data_i = ((intptr_t)data + V_LEN) & -V_LEN;
#endif
    const char *aligned_data = (const char *)aligned_data_i;

    if (aligned_data >= data + len) {
        // The input wasn't big enough to fill one vector
        if (NAME(z_validate_small_chunk)(state, data, len, 0, 0))
            return 0;
    } else {
        // Validate the start section between data and aligned_data
        if (NAME(z_validate_small_chunk)(state, data, aligned_data - data, 0, 0))
            return 0;

        // Get the size of the aligned inner section of data
        size_t aligned_len = len - (aligned_data - data);
#if USE_NEXT_LOAD
        // Make sure there are V_LEN-1 bytes at the end, as we load past the end
        aligned_len = aligned_len > V_LEN - 1 ? aligned_len - (V_LEN - 1) : 0;
#endif
        // Subtract from the aligned_len any bytes at the end of the input that
        // don't fill an entire UNROLL_SIZE-byte chunk
        aligned_len -= (aligned_len % UNROLL_SIZE);

        // Only make the first load if the rounding above leaves us enough room
        if (aligned_len > 0)
            NAME(load_first)(state, aligned_data, aligned_data - data);

        // Validate the main inner part of the input, in UNROLL_SIZE-byte chunks
        for (size_t offset = 0; offset < aligned_len; offset += UNROLL_SIZE) {
            if (NAME(z_validate_unrolled_chunk)(state, aligned_data + offset))
                return 0;
        }

        // Validate the end section. This might be multiple vector's worth of
        // data, due to the main loop being unrolled
        const char *end_data = aligned_data + aligned_len;
        size_t end_len = (data + len) - end_data;
        for (size_t offset = 0; offset < end_len; offset += V_LEN) {
            size_t this_len = end_len - offset;
            this_len = this_len > V_LEN ? V_LEN : this_len;
            if (NAME(z_validate_small_chunk)(state, end_data + offset, this_len,
                        end_data + offset - data, 1))
                return 0;
        }
    }

    // Micro-optimization compensation! We have to double check for a multi-byte
    // sequence that starts on the last byte, since we check for the first
    // continuation byte using error masks, which are shifted one byte forward
    // in the data stream. Thus, a leader byte in the last position will be
    // ignored if it's also the last byte of a vector.
    if (len > 0 && (uint8_t)data[len - 1] >= 0xC0)
        return 0;

#if USE_NEW_VECTOR_CONT_CHECK
    state->last_bytes = state->bytes;
#endif

    // The input is valid if we don't have any more expected continuation bytes
    return !test_carry_req(state);
}

// Undefine all macros, unless NO_UNDEF is defined. This is a hacky way to
// make integration of this library a bit easier for some uses, where the
// user wants to use something like the NAME macro or the vector/mask types.

#if !defined(NO_UNDEF)

#undef NAME
#undef ASCII_SUFFIX
#undef SUFFIX
#undef V_LEN

#undef vec_t
#undef vmask_t
#undef vmask2_t

#undef v_load
#undef v_loadu
#undef v_set1
#undef v_and
#undef v_andn
#undef v_or
#undef v_xor
#undef v_add
#undef v_shl
#undef v_shr
#undef v_test_any
#undef v_test_bit
#undef v_lookup
#undef V_TABLE_16
#undef V_TABLE_64

#undef vmask_zero
#undef mask_carry_req
#undef test_carry_req
#undef result_fails

#undef USE_NEXT_LOAD
#undef USE_UNALIGNED_LOADS
#undef USE_VECTOR_CONT_CHECK
#undef UNROLL_COUNT
#undef UNROLL_SIZE

#endif
